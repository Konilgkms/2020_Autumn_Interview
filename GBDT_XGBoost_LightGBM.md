从面经中可以看出，XGBoost和LightGBM是一个常考点，整理一些在面试中遇到的知识点。

### GBDT（Gradient Boost Decision Tree）
GBDT每一次的计算是都为了减少上一次的残差，进而在残差减少（负梯度）的方向上建立一个新的模型。
#### 1.GBDT将决策树作为基学习器
- 初始化使得最小化损失函数
- 前向分布算法，每次迭代用基学习器取拟合整个函数的最速下降方向，再求最小化当前损失的线性搜索步长。将得到的基学习器以及步长放入加性模型中就得到本次迭代的模型
- 经过M次迭代后求和出来的基学习器的组合就是最后的模型
- 为了抵抗过拟合，对每次迭代增加收缩系数作为正则化参数
#### 2.不同于Adaboost修改权重，GBDT从初始化的函数开始出发，每步朝损失函数下降最快的方向——梯度方向优化
#### 3.Gradient Boosting 框架
- 初始化：最小化损失函数
- 对于m = 1 到 M 次迭代中：
- 对于i = 1...N，计算当前的最速下降方向，即损失函数对于当前F的偏导数
- 得到最速下降方向后，代入损失函数，在最小化损失函数的目标下，计算搜索步长
- 得到方向与步长后，更新F
- 得到M个F后相加，得到最终的模型

### XGBoost（Extreme Gradient Boosting）
#### 1.与GBDT相比较：
- 策略上是相似的，都是聚焦残差
- GBDT旨在通过不断加入新的树，在最快速度下降低残差
- XGBoost则可以人为定义损失函数，只需得到损失函数对于参数的一阶、二阶导数即可进行boosting
- XGBoost进一步增大了泛化能力，贪婪寻找添加树的结构，针对Loss Function中的损失函数进入惩罚项
- GBDT只用了目标函数的一阶信息，让新学习器拟合上一轮目标函数的负梯度方向，梯度下降法
- XGBoost使用目标函数的二阶信息，牛顿法
#### 2.三个特点：
- 惩罚项依赖于叶子结点的数量T以及每个叶节点的权重L2范数
- 每次迭代改变学习率，降低对每个学习器的信任度
- 特征列子采样，增加随机性
#### 3.XGBoost步骤：
- 第一步：初始化每个样本的预测值
##### 循环建树，直到满足停止条件
- 第二步：定义目标函数
- 第三步：目标函数的泰勒化简
- 第四步：基于决策树的目标函数的终极化简，得到导数
- 第五步：根据最优切分点划分算法来建立决策树
- 第六步：利用新的决策树预测样本值，并累加到原来的值上
#### 4.XGBoost相比GBDT而言有哪些优势：
- 精度更高：GBDT只用到一阶泰勒， 而xgboost对损失函数进行了二阶泰勒展开， 一方面为了增加精度， 另一方面也为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数。
- 灵活性更强：GBDT以CART作为基分类器，而Xgboost不仅支持CART，还支持线性分类器，另外，Xgboost支持自定义损失函数，只要损失函数有一二阶导数。
- 正则化：xgboost在目标函数中加入了正则，用于控制模型的复杂度。有助于降低模型方差，防止过拟合。正则项里包含了树的叶子节点个数，叶子节点权重的L2范式。
- Shrinkage（缩减）：相当于学习速率。这个主要是为了削弱每棵树的影响，让后面有更大的学习空间，学习过程更加的平缓
- 列抽样：这个就是在建树的时候，不用遍历所有的特征了，可以进行抽样，一方面简化了计算，另一方面也有助于降低过拟合
- 缺失值处理：这个是xgboost的稀疏感知算法，加快了节点分裂的速度
- 并行化操作：块结构可以很好的支持并行计算

### LightGBM
#### 1.影响XGBoost寻找最优分裂点的复杂度的因素：
- 样本数量
- 分裂点数量
- 特征数量
#### 2.对比XGBoost，LightGBM在性能上作出的改进：
- 减少样本数量：单边梯度抽样
- 减少分裂点数量：直方图
- 减少特征数量：互斥特征捆绑算法
#### 3.直方图算法Histogram
- LightGBM的直方图算法是代替Xgboost的预排序算法的
- 直方图算法说白了就是把连续的浮点特征离散化为k个整数，并根据特征所在的bin对其进行梯度累加和个数统计，在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点
- Histogram还可以进一步加速。一个叶子节点的Histogram可以直接由父节点的Histogram和兄弟节点的Histogram做差得到。一般情况下，构造Histogram需要遍历该叶子上的所有数据，通过该方法，只需要遍历Histogram的k个捅。速度提升了一倍
- Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在实际的数据集上表明，离散化的分裂点对最终的精度影响并不大，甚至会好一些。原因在于decision tree本身就是一个弱学习器，分割点是不是精确并不是太重要，采用Histogram算法会起到正则化的效果，有效地防止模型的过拟合（bin数量决定了正则化的程度，bin越少惩罚越严重，欠拟合风险越高）。 直方图算法可以起到的作用就是可以减小分割点的数量，加快计算。
#### 4.单边梯度抽样算法GOSS
- 单边梯度抽样算法(Gradient-based One-Side Sampling)是从减少样本的角度出发， 排除大部分权重小的样本，仅用剩下的样本计算信息增益，它是一种在减少数据和保证精度上平衡的算法。
#### 5.互斥特征捆绑算法EFB
- 高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥，可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。
#### 6.Level-wise 与 Leaf-wise
- XGBoost在树的生成过程中采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。
- Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销
- Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。
- Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

参考/推荐阅读：
https://mp.weixin.qq.com/s/NC9CwR4cfDUJ26WpHsvkPQ
https://mp.weixin.qq.com/s/BIHr5GDunm2U-Szs0Dt32w
